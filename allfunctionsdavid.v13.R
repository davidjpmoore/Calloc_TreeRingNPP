require(zoo)require(fields)require(pspline)################################################################################################# extracthadcru################################################################################################extracthadcru <- function(x,maxlat,minlat,maxlon,minlon) { 	forextraction <- outer(seq(maxlat,minlat,by=-5), seq(minlon,maxlon,by=5), f <-function(x,y)(paste(x,"_",y, sep="")))	extracteddataset <- x[,c(forextraction)]}################################################################################################# loadhadcru################################################################################################loadhadcru <- function() {setwd("/Users/frank/Desktop/frank/metdatathings/cru5x5")HadCRUT3v <- scan(file="hadcrut3v.dat", sep="",skip=0, na.strings="-1.000e+30")HadCRUT3v.matrix <- matrix(HadCRUT3v, ncol=2592,byrow=TRUE)HadCRUT3v.ts <- ts(HadCRUT3v.matrix, start=c(1850,1), frequency=12)gridnames <- outer(seq(87.5,-87.5,by=-5), seq(-177.5,177.5,by=5), f <-function(x,y)(paste(x,"_",y, sep=""))) gridnames <- as.vector(t(gridnames))  # can use to rename columnscolnames(HadCRUT3v.ts) <- gridnamesreturn(HadCRUT3v.ts)}################################################################################################# davidtsplot################################################################################################davidtsplot <-  function(x,lty=1,lwd=1,xlab="Year",ncol=1, col=tim.colors(ncol(x)),plotlegend=TRUE,y.intersp=.8, x.intersp=.8,...) {ts.plot(x,lty=lty,lwd=lwd,xlab=xlab,col=col,...)if(plotlegend) legend(locator(1),colnames(x),lty=lty,col=col,lwd=lwd,ncol=ncol, y.intersp = y.intersp, x.intersp= x.intersp)}################################################################################################# compile chronologies# reads in tabs data from multiple files and compiles all of the relevant sections###############################################################################################compilechronologies<- function(datasetnames) {datasetnames.tabs <- paste(datasetnames,"_tabs",sep="")for (i in 1:length(datasetnames.tabs)) {	assign(datasetnames.tabs[i], importtabsfromARSTAN(datasetnames.tabs[i]))}chronologies.num <- NULLchronologies.seg <- NULLchronologies.age <- NULLchronologies.raw <- NULLchronologies.std <- NULLchronologies.res <- NULLchronologies.ars <- NULLrcs.num <- NULLrcs.rcs <- NULLrcs.sig <- NULLrcs.crv <- NULLfor (i in 1:length(datasetnames.tabs)) {		temp <- get(datasetnames.tabs[i])				chronologies.num <- ts.union(chronologies.num,temp$chronologystuff[,1])		chronologies.seg <- ts.union(chronologies.seg,temp$chronologystuff[,2])		chronologies.age <- ts.union(chronologies.age,temp$chronologystuff[,3])		chronologies.raw <- ts.union(chronologies.raw,temp$chronologystuff[,4])		chronologies.std <- ts.union(chronologies.std,temp$chronologystuff[,5])		chronologies.res <- ts.union(chronologies.res,temp$chronologystuff[,6])		chronologies.ars <- ts.union(chronologies.ars,temp$chronologystuff[,7])		rcs.num <- ts.union(rcs.num,temp$RCSstuff[,1])		rcs.rcs <- ts.union(rcs.rcs,temp$RCSstuff[,2])		rcs.sig <- ts.union(rcs.sig,temp$RCSstuff[,3])		rcs.crv <- ts.union(rcs.crv,temp$RCSstuff[,4])			}colnames(chronologies.num) <- datasetnamescolnames(chronologies.seg) <- datasetnamescolnames(chronologies.age) <- datasetnamescolnames(chronologies.raw) <- datasetnamescolnames(chronologies.std) <- datasetnamescolnames(chronologies.res) <- datasetnamescolnames(chronologies.ars) <- datasetnamesif (is.integer(ncol(rcs.num)))	{	colnames(rcs.num) <- datasetnames	colnames(rcs.rcs) <- datasetnames	colnames(rcs.sig) <- datasetnames	colnames(rcs.crv) <- datasetnames}outputlist <- list(chronologies.num = chronologies.num, chronologies.seg = chronologies.seg,chronologies.age = chronologies.age,chronologies.raw = chronologies.raw,chronologies.std = chronologies.std,chronologies.res = chronologies.res,chronologies.ars = chronologies.ars,rcs.num = rcs.num,rcs.rcs = rcs.rcs,rcs.sig = rcs.sig,rcs.crv = rcs.crv)return (outputlist)}###########################################################################################function importtabsfromARSTAN##reads in arstan tabs files and makes a list for the chronology part and the RCS part##########################################################################################importtabsfromARSTAN <- function(x) {		tempimport <- scan(file=x,sep = "\t", skip=1, na.strings=c("-9.9900","-9.990","-9.99"))	tempimportnames <- scan(file=x,sep = "\t", skip=0, na.strings="-9.9900",nlines=1, what="character")		treeringdata <- matrix(tempimport,ncol=length(tempimportnames), byrow=TRUE)	colnames(treeringdata) <- tempimportnames	tstreeringdata <- ts(treeringdata[,-1], start=treeringdata[1,1])		if (length(tempimportnames)==13)	{	RCSstuff <- coredata(tstreeringdata[,8:12])	RCSstuff <- ts(RCSstuff[,-1], start=RCSstuff[1,1])	chronologystuff <- tstreeringdata[,1:7]	}		else {	chronologystuff <- tstreeringdata	RCSstuff <- NULL	}	output <- list(chronologystuff=chronologystuff,RCSstuff=RCSstuff)return(output)}###########################################################################################function writemts## writes multipletimeseries to a file##########################################################################################writemts <- function(mts,file="outfile",sigdigits=4) {	temp <- cbind(index(mts),round(mts,sigdigits))	ifelse(is.null(ncol(mts)), (colnames(temp) <- c("Year",substitute(mts))),(colnames(temp) <- c("Year",colnames(mts))))	write.table(temp,file,quote=FALSE,row.names=FALSE,sep="	")}###########################################################################################function monthlycorrelobservationoverlap## efficient !##########################################################################################monthlycorrelobservationoverlap <- function(x) {	presencematrix <- 0*(as.matrix(x))+1	presencematrix[is.na(presencematrix)] <- 0	overlapmatrix <- t(presencematrix[,-1])%*%presencematrix[,1]	overlapmatrix}###########################################################################################function fancymonthlycorrels##can use something like this to select what should be plotted#barplot(a$correlations[c(13:31)],names.arg=c(colnames(a$correlations)[c(13:31)]))###########################################################################################fancymonthlycorrels <- function(treeringdata,monthlydata,startyear,endyear) {temp<- ts.union(treeringdata,lag(monthlydata,-1),monthlydata)colnames(temp) <- c("treeringdata",paste("p",colnames(monthlydata),sep=""),colnames(monthlydata))temp <- window(temp,startyear,endyear)seasons <- cbind(rowMeans(temp[,2:13]),rowMeans(temp[,7:9]),rowMeans(temp[,5:10]),rowMeans(temp[,16:18]),rowMeans(temp[,18:19]),rowMeans(temp[,17:22]),rowMeans(temp[,18:20]),rowMeans(temp[,19:21]),rowMeans(temp[,19:20]),rowMeans(temp[,20:21])) # finish to include seasonal means,etccolnames(seasons) <- c("pYear","pJJA","pAMJJAS","MAM","MJ", "AMJJAS","MJJ","JJA","JJ","JA")seasons <- ts(seasons,start=start(temp)[1])temp2 <- ts.union(temp,seasons)colnames(temp2) <- c(colnames(temp),colnames(seasons))correlations <- cor(temp2[,1],temp2[,2:ncol(temp2)],use="pairwise.complete.obs")nvalues <- monthlycorrelobservationoverlap(temp2)print(temp2)return(correlations)}###########################################################################################function cambial.age.array##########################################################################################cambial.age.array <- function(treeringdata, pith.offset=0) {if (length(pith.offset)==1) pith.offset <- rep(pith.offset,ncol(treeringdata))stopifnot (length(pith.offset)== ncol(treeringdata))years <- function(temp) {(temp*0+1)*time(temp)}fyos<- function(x) {		L <- !is.na(x)        idx <- c(NA, which(L))[cumsum(L) + 1]		fy <- min(idx,na.rm=TRUE)		return(fy)		}combined <- function(y) {(years(y)-fyos(y)+1)}rel.years <- apply(treeringdata,2,combined)rel.years <- scale(rel.years,center=-pith.offset,scale=FALSE)	if(is.ts(treeringdata)) (rel.years <- ts(rel.years, start=start(treeringdata)))	return(rel.years)}###########################################################################################function tssubset###     #example1#ts.plot(tssubset(testing,cambial.age(testing)>49&cambial.age(testing)<60),type="p",col=1)##     #example2#ca <- cambial.age(testing)#tssubset(testing,ca>30&ca<50)#############################################################################################tssubset <- function(x,yandconditon){temp <- eval(yandconditon)temp[temp==FALSE]<- NAx.subset <- x*tempcolnames(x.subset) <- colnames(x)return(x.subset)}###########################################################################################function ts.cor##########################################################################################ts.cor <-function(x,y) {temp <- ts.union(x,y)temp.cor <- cor(temp,use="pairwise.complete.obs")temp.cor <- temp.cor[1:ncol(x),(ncol(x)+1):ncol(temp)]rownames(temp.cor) <- colnames(x)colnames(temp.cor) <- colnames(y)return(temp.cor)}###########################################################################################function EVD##########################################################################################EVD <- function(x,window=50) {aa.x <- agealign2(x,1)aa.x <- tspadding(aa.x,window)aa.presence <- runningpresence(aa.x,window)aa.x.runningsd <- rapply(aa.x,window,sd,by=1,na.pad=TRUE,na.rm=TRUE)aa.x.runningsd[aa.presence<window/2]<-NAaa.x.runningsd.mean <- makechronology(aa.x.runningsd)EVD.x.detrended.aa <- aa.x/aa.x.runningsd.meanEVD.x.detrended.final <- agealign2(EVD.x.detrended.aa,fy(x))EVD.x.detrended.final.crnl <- makechronology(EVD.x.detrended.final)ts.plot(EVD.x.detrended.final.crnl)return(EVD.x.detrended.final.crnl)}###########################################################################################function RCS##########################################################################################RCS <- function(dataset, smoothspl=40) {aa <- agealign2(dataset,1)aa.crn <- makechronology(aa)aa.crn.smth <- splinesmoother2(aa.crn,smoothspl)finished <- apply(aa,2,function(x){(x/aa.crn.smth)})finished <- ts(finished)cal.detrended <- agealign2(finished,fy(dataset))rcs.crnl<- makechronology(cal.detrended)return(aa,aa.crn,aa.crn.smth,finished,cal.detrended,rcs.crnl)}###########################################################################################function spline.det##########################################################################################spline.det <- function(x,smoothing) {spline.p.for.smooth.Pspline<-function(year)  {	p <- .5/((6*(cos(2*pi/year)-1)^2/(cos(2*pi/year)+2)))	return(p)	}ifelse ((is.null(ncol(x))),x2 <- ts.union(x,x), x2 <- x)fyarray <- start(x2)[1]lyarray <- end(x2)[1]begin <- fy(x2)end <- ly(x2)smoothedarray <- x2spline.p <- spline.p.for.smooth.Pspline(smoothing)for (i in 1:ncol(x2)) {temp <- smooth.Pspline((begin[i]:end[i]),x2[(begin[i]-fyarray+1):(end[i]-fyarray+1),i],spar=spline.p,method=1)smoothedarray[(begin[i]-fyarray+1):(end[i]-fyarray+1),i]  <- temp$ysmth}if (is.null(ncol(x))) {smoothedarray <- smoothedarray[,-1]}residualarray <- x-smoothedarrayratioarray <- x/smoothedarrayreturn(x,smoothedarray,residualarray,ratioarray)}###########################################################################################function continuous2month##########################################################################################continuous2month <- function(continuousmonthlymetdata) {for (i in 1:12) assign(paste("temp",i,sep=""),ts(continuousmonthlymetdata[cycle(continuousmonthlymetdata)==i],start=start(continuousmonthlymetdata)[1]))monthmatrix <- ts.union(temp1,temp2,temp3,temp4,temp5,temp6,temp7,temp8,temp9,temp10,temp11,temp12)colnames(monthmatrix)<-c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")return(monthmatrix)}###########################################################################################function normalizevariancewindow##########################################################################################normalizevariancewindow <- function(x, start, end) {centered1 <- scale(x,center=apply(window(x,start,end),2,mean,na.rm=TRUE),scale=FALSE)centered.means <- unlist(attributes(centered1)["scaled:center"])scaled1 <- scale(centered1,center=FALSE,scale=apply(window(centered1,start,end),2,sd,na.rm=TRUE))scaled2 <- scale(scaled1,center=-centered.means,scale=FALSE)return(scaled2)}#####################################################################################################################################################################################function stabilizevariance4nhweightedrecon##used in attempt to add a weighted mean for the variance stabilized nh chronols#does not give anything for biweight means back##########################################################################################stabilizevariance4nhweightedrecon <- function(x,WL) {	runningrforSTABILIZEVARIANCE <-function (data, win.length=WL) {		cor.mat <- cor(data, use="pairwise.complete.obs")		diag(cor.mat) <- NA		obs.overlap.mat <- observationoverlap(data)		cor.mat[obs.overlap.mat < (win.length/3)] <- NA		MEAN.R.trunc <- mean(cor.mat, na.rm=TRUE)	return(MEAN.R.trunc)	}mean.x <- mean(makechronology(x),na.rm=TRUE)  #pre-process the data to have a mean of zerox <- x-mean.x  x.presence <- (x*0+1)lat2rounded <- c(60,40,60,60,40,60,60,40,60,60,40,60,60,60)cosweights <- cos((1/180*pi)*lat2rounded)cosweights<-round(cosweights,2)weightseries <- scale(x.presence,center=FALSE,scale=c(1/cosweights))rowSums(weightseries,na.rm=TRUE) -> sumofweightssumofweights[sumofweights==0]<-NArowSums(x*weightseries,na.rm=TRUE) -> weightedsitesumweightedsitesum/sumofweights->latitudeweightedaveragechronolts(latitudeweightedaveragechronol,start=start(x))->tslatitudeweightedaveragechronolchronology <- tslatitudeweightedaveragechronolsamplesize <- tssampledepth(x)correlmatrix <- cor(x,use="pairwise.complete.obs") # could insert code for using only correls with certain observation overlapdiag(correlmatrix) <- NAMEAN.R <- mean(correlmatrix, na.rm =TRUE)rbar.running <- rapply(x,WL,runningrforSTABILIZEVARIANCE,by=1,by.column=FALSE,na.pad=TRUE)rbar.running <- na.locf(rbar.running,na.rm=FALSE)rbar.running <- rev(na.locf(rev(rbar.running),na.rm=FALSE))rbar.running[samplesize==0] <- NAeffsamplesize <- samplesize/(1+(samplesize-1)*rbar.running)effsamplesize <- pmin(effsamplesize,samplesize,na.rm=TRUE) # takes care of setting the effsamplesize to 1 when sampledepth=1# and also if rbar goes negative effsamplesize gets larger than samplesize, and this brings it back down.simpleeffsamplesize <- samplesize/(1+(samplesize-1)*MEAN.R)RUNNINGadjustedchronology <- chronology*(effsamplesize*MEAN.R)^.5SIMPLEadjustedchronology <- chronology*(simpleeffsamplesize*MEAN.R)^.5everything <- ts.union(chronology,SIMPLEadjustedchronology,RUNNINGadjustedchronology,samplesize,effsamplesize,rbar.running)everything <- scale(everything,center=c(rep(-mean.x,3),rep(0,3)),scale=FALSE)#add back the mean to the datasetreturn(everything)}#########################################################################################################################################################################################################################################################################################################################################################################function newsplineforgaps##as for splinesmoother, but deals with gaps##########################################################################################newsplineforgaps <- function(x,smoothing) {spline.p.for.smooth.Pspline<-function(year)  {	p <- .5/((6*(cos(2*pi/year)-1)^2/(cos(2*pi/year)+2)))	return(p)	}smoothedarray <- xspline.p <- spline.p.for.smooth.Pspline(smoothing)for (i in 1:ncol(x)) {temp.data <- x[,i]year.test <- !is.na(temp.data)valid.years <-index(temp.data)[year.test]temp <- smooth.Pspline(valid.years,temp.data[year.test],spar=spline.p,method=1)smoothedarray[year.test,i]  <- temp$ysmth}return(smoothedarray)}###########################################################################################function splinesmoother#As for tree-ring type splines#Spar is caluclated for smooth.Pspline as .5/(formula in Cook and Peters 1981),derived by#trial and error. Tested for ranges of at least between 20 and 200. requires package("pspline")#tested on version 1.0-8##########################################################################################splinesmoother <- function(x,smoothing) {spline.p.for.smooth.Pspline<-function(year)  {	p <- .5/((6*(cos(2*pi/year)-1)^2/(cos(2*pi/year)+2)))	return(p)	}fyarray <- start(x)[1]lyarray <- end(x)[1]begin <- fy(x)end <- ly(x)smoothedarray <- xspline.p <- spline.p.for.smooth.Pspline(smoothing)for (i in 1:ncol(x)) {temp <- smooth.Pspline((begin[i]:end[i]),x[(begin[i]-fyarray+1):(end[i]-fyarray+1),i],spar=spline.p,method=1)smoothedarray[(begin[i]-fyarray+1):(end[i]-fyarray+1),i]  <- temp$ysmth}return(smoothedarray)}###########################################################################################function splinesmoother2##as for splinesmoother, but deals with singlecolumns##########################################################################################splinesmoother2 <- function(x,smoothing) {spline.p.for.smooth.Pspline<-function(year)  {	p <- .5/((6*(cos(2*pi/year)-1)^2/(cos(2*pi/year)+2)))	return(p)	}ifelse ((is.null(ncol(x))),x2 <- ts.union(x,x), x2 <- x)fyarray <- start(x2)[1]lyarray <- end(x2)[1]begin <- fy(x2)end <- ly(x2)smoothedarray <- x2spline.p <- spline.p.for.smooth.Pspline(smoothing)for (i in 1:ncol(x2)) {temp <- smooth.Pspline((begin[i]:end[i]),x2[(begin[i]-fyarray+1):(end[i]-fyarray+1),i],spar=spline.p,method=1)smoothedarray[(begin[i]-fyarray+1):(end[i]-fyarray+1),i]  <- temp$ysmth}if (is.null(ncol(x))) {smoothedarray <- smoothedarray[,-1]}return(smoothedarray)}###########################################################################################function Mean.R############################################################################################Mean.R <- function(x,N=30) {correlmatrix <- cor(x,use="pairwise.complete.obs") # could insert code for using only correls with certain observation overlapdiag(correlmatrix) <- NAwindowoverlap <- observationoverlap(x)correlmatrix[windowoverlap<N]<-NAMEAN.R.result <- mean(correlmatrix, na.rm =TRUE)return(MEAN.R.result)}###########################################################################################function runningpresence############################################################################################runningpresence <- function(x,windowlength) {rollapply(x*0+1,windowlength,sum,na.pad=TRUE,na.rm=TRUE)}###########################################################################################function tspadding############################################################################################tspadding <- function(x,n.years) {x.beg<- tsp(x)[1]x.end <- tsp(x)[2]	temp <- ts(rep(NA,x.end-x.beg+n.years*2+1),start=(x.beg-n.years))	temp <- ts.union(temp,x)	temp<-temp[,-1]	if (is.integer(ncol(x))) colnames(temp)<- colnames(x)return(temp)}###########################################################################################function meanandvarianceadjust##should be ok, but doublecheck.##########################################################################################meanandvarianceadjust <- function(originaldataset, targetdataset, startwindow,endwindow) {target_sd <- sd(window(targetdataset,startwindow,endwindow))target_mean <- mean(window(targetdataset,startwindow,endwindow))merged <- ts.union(targetdataset,originaldataset)scale.data <- apply(window(merged,startwindow,endwindow),2,sd)/target_sdtemp <- scale(merged,scale=scale.data,center=FALSE)temp <- ts(temp,start=start(merged)[1])center.data <- apply(window(temp,startwindow,endwindow),2,mean)-target_meantemp2 <- scale(temp,scale=FALSE,center=center.data)temp2 <- ts(temp2,start=start(temp)[1])temp3 <- temp2[,-1]if (is.numeric(ncol(originaldataset))) (colnames(temp3)<-colnames(originaldataset))print(scale.data)print(center.data)return(temp3)}###########################################################################################function meanandvarianceadjust2##should be ok, but doublecheck.##########################################################################################meanandvarianceadjust2 <- function(originaldataset, targetdataset, startwindow,endwindow) {target_sd <- sd(window(targetdataset,startwindow,endwindow))target_mean <- mean(window(targetdataset,startwindow,endwindow))merged <- ts.union(targetdataset,originaldataset)scale.data <- apply(window(merged,startwindow,endwindow),2,sd)/target_sdtemp <- scale(merged,scale=scale.data,center=FALSE)temp <- ts(temp,start=start(merged)[1])center.data <- apply(window(temp,startwindow,endwindow),2,mean)-target_meantemp2 <- scale(temp,scale=FALSE,center=center.data)temp2 <- ts(temp2,start=start(temp)[1])temp3 <- temp2[,-1]if (is.numeric(ncol(originaldataset))) (colnames(temp3)<-colnames(originaldataset))output <- NULLoutput$values <- temp3output$scale <- scale.data[2]output$center <- center.data[2]return(output)}###########################################################################################function agealign2##slow,but works##########################################################################################agealign2 <- function(x,age.vector) {if (length(age.vector)==1) age.vector <- rep(age.vector,ncol(x))stopifnot (length(age.vector)== ncol(x))fy.vector <- fy(x)ly.vector <- ly(x)result <- ts(window(x[,1],start=fy.vector[1],end=ly.vector[1]),start=age.vector[1])for (i in 2:ncol(x)) {	temp1 <- ts(window(x[,i],start=fy.vector[i],end=ly.vector[i]),start=age.vector[i])	result <- ts.union(result,temp1)	}colnames(result) <- colnames(x)return(result)}###########################################################################################function agealign  ##slow,but works##########################################################################################agealign <- function(x) {fy.vector <- fy(x)ly.vector <- ly(x)result <- ts(window(x[,1],start=fy.vector[1],end=ly.vector[1]),start=1)for (i in 2:ncol(x)) {	temp1 <- ts(window(x[,i],start=fy.vector[i],end=ly.vector[i]),start=1)	result <- ts.union(result,temp1)	}colnames(result) <- colnames(x)return(result)}#image(d,col=rev(rainbow(50,start=0,end=4/6,gamma=1))) ###########################################################################################function beamplot##########################################################################################beamplot <- function(x, ...) {x.presence <- (x*0+1)xseries <- scale(x.presence,center=FALSE,scale=c(1/(1:ncol(x))))ts.plot(xseries, ...)}#####################################################################################################################################################################################function newbiweightrobustmean##if using on a data.frame call function as: biweightrobustmean(as.matrix(x))#this seems to be slower than the older version ....shit ;-)##########################################################################################newbiweightrobustmean <- function(x) {	tempmean <- rep(NA,nrow(x))	sampledepth<-rowSums(0*x+1, na.rm=TRUE)	rawmedian<- apply(x,1,median,na.rm=TRUE)	x_norm <- (x-rawmedian)/apply(x,1,mad,na.rm=TRUE) # here using median absolute deviation and initial estimate with median instead, maybe more robust	for (i in 1:3) {		x_norm[abs(x_norm) > 6.08] <- 6.08      # special consideration for any extreme value over 6.08 stdev away to get weight of 6.08 stdev which is zero		xweights <- ((1-(x_norm/6.08)^2)^2)				weightsums <-rowSums(xweights,na.rm=TRUE)		weightsums[weightsums==0]<-NA		tempmean <- rowSums(x*xweights,na.rm=TRUE)/weightsums							x_norm <- (x-tempmean)/apply(x,1,mad, na.rm=TRUE) # here using median absolute deviation	}	tempmean[which(sampledepth==1)]<-rawmedian[which(sampledepth==1)]	ifelse(is.ts(x)==TRUE, robustmean <- ts(tempmean, start = start(x)[1]), robustmean <- tempmean)robustmean}###############fy <- function(object) {fyos<- function(x) {		L <- !is.na(x)        idx <- c(NA, which(L))[cumsum(L) + 1]		fy <- min(idx,na.rm=TRUE)		return(fy)		}ifelse (is.mts(object), 	rel.years <- apply(object,2,fyos),	rel.years <- fyos(object))	calyears <- rel.years+start(object)[1]-1return(calyears)}##############################ly <- function(object) {fyos<- function(x) {		L <- !is.na(x)        idx <- c(NA, which(L))[cumsum(L) + 1]		fy <- max(idx,na.rm=TRUE)}ifelse (is.mts(object), 	rel.years <- apply(object,2,fyos),	rel.years <- fyos(object))	calyears <- rel.years+start(object)[1]-1return(calyears)}#########################################################################################################################wooooo function sorted beam plot#########################################################################################################sortedbeamplot <- function(x,direction=c("none","ascending","descending"),datareturn=FALSE, ...) {direction <- match.arg(direction)	columnsorter <- function(object, columnindexer) {		if (ncol(object)!=length(columnindexer)) stop("length of column vector does not equal number of columns")		new.ind <- sort(columnindexer,index.return=TRUE)		x <- x[,c(new.ind$ix)]	}if (direction=="ascending") {x.ly <- ly(x)x <-  columnsorter(x,x.ly)x.fy <- fy(x)x <-  columnsorter(x,x.fy)}if (direction=="descending") {x.fy <- fy(x)x <-  columnsorter(x,x.fy)x.ly <- ly(x)x <-  columnsorter(x,x.ly)}if (direction=="none") {beamplot(x, ...)}else {beamplot(x, ...)}if (datareturn) return(x)}####################################################################################################################################################################################################function writemeasformat !!!!!!##########################################################################################writemeasformat <- function(x) {seriesnames <- strtrim(dimnames(x)[[2]],8)for (j in 1:ncol(x)) {	temp.x <- na.contiguous(x[,j])	begining.year <- start(temp.x)[1]	year <- begining.year	for (i in 1:length(temp.x)) {		if (i==1) cat(encodeString(seriesnames[j],w=8,justify="left"),encodeString(year,w=4,justify="right"),sep="")		if(year%%10!=9) cat(noquote(encodeString(temp.x[i],w=6,justify="right")))		if (year%%10==9) {			cat(noquote(encodeString(temp.x[i],w=6,justify="right")))			cat("\n",encodeString(seriesnames[j],w=8,justify="left"),encodeString(year+1,w=4,justify="right"),sep="")			}		if (i==length(temp.x)) cat("","-9999","\n")		year=year+1}}}###########################################################################################function stabilizevariance5nhweightedrecon#(requires zoo) ##########################################################################################stabilizevariance5nhweightedrecon <- function(x,WL) {	runningrforSTABILIZEVARIANCE <-function (data, win.length=WL) {		cor.mat <- cor(data, use="pairwise.complete.obs")		diag(cor.mat) <- NA		obs.overlap.mat <- observationoverlap(data)		cor.mat[obs.overlap.mat < (win.length/3)] <- NA		MEAN.R.trunc <- mean(cor.mat, na.rm=TRUE)	return(MEAN.R.trunc)	}mean.x <- mean(makechronology(x),na.rm=TRUE)  #pre-process the data to have a mean of zerox <- x-mean.x  x.presence <- (x*0+1)lat2rounded <- c(60,50,60,60,50,60,60,50,60,60,50,60,60,60)cosweights <- cos((1/180*pi)*lat2rounded)cosweights<-round(cosweights,2)weightseries <- scale(x.presence,center=FALSE,scale=c(1/cosweights))rowSums(weightseries,na.rm=TRUE) -> sumofweightssumofweights[sumofweights==0]<-NArowSums(x*weightseries,na.rm=TRUE) -> weightedsitesumweightedsitesum/sumofweights->latitudeweightedaveragechronolts(latitudeweightedaveragechronol,start=start(x))->tslatitudeweightedaveragechronolchronology <- tslatitudeweightedaveragechronolsamplesize <- tssampledepth(x)correlmatrix <- cor(x,use="pairwise.complete.obs") # could insert code for using only correls with certain observation overlapdiag(correlmatrix) <- NAMEAN.R <- mean(correlmatrix, na.rm =TRUE)rbar.running <- rollapply(x,WL,runningrforSTABILIZEVARIANCE,by=1,by.column=FALSE,na.pad=TRUE)rbar.running <- na.locf(rbar.running,na.rm=FALSE)rbar.running <- rev(na.locf(rev(rbar.running),na.rm=FALSE))rbar.running[samplesize==0] <- NAeffsamplesize <- samplesize/(1+(samplesize-1)*rbar.running)effsamplesize <- pmin(effsamplesize,samplesize,na.rm=TRUE) # takes care of setting the effsamplesize to 1 when sampledepth=1# and also if rbar goes negative effsamplesize gets larger than samplesize, and this brings it back down.simpleeffsamplesize <- samplesize/(1+(samplesize-1)*MEAN.R)RUNNINGadjustedchronology <- chronology*(effsamplesize*MEAN.R)^.5SIMPLEadjustedchronology <- chronology*(simpleeffsamplesize*MEAN.R)^.5everything <- ts.union(chronology,SIMPLEadjustedchronology,RUNNINGadjustedchronology,samplesize,effsamplesize,rbar.running)everything <- scale(everything,center=c(rep(-mean.x,3),rep(0,3)),scale=FALSE)#add back the mean to the datasetreturn(everything)}###########################################################################################function stabilizevariance3#(requires zoo) includes automatic removal of dataset mean in comparison to stablizevariance2##########################################################################################stabilizevariance3 <- function(x,WL) {	runningrforSTABILIZEVARIANCE <-function (data, win.length=WL) {		cor.mat <- cor(data, use="pairwise.complete.obs")		diag(cor.mat) <- NA		obs.overlap.mat <- observationoverlap(data)		cor.mat[obs.overlap.mat < (win.length/3)] <- NA		MEAN.R.trunc <- mean(cor.mat, na.rm=TRUE)	return(MEAN.R.trunc)	}mean.x <- mean(makechronology(x),na.rm=TRUE)  #pre-process the data to have a mean of zerox <- x-mean.x  samplesize <- tssampledepth(x)chronology <- makechronology(x)biwgt.chronology <- biweightrobustmean(x)correlmatrix <- cor(x,use="pairwise.complete.obs") # could insert code for using only correls with certain observation overlapdiag(correlmatrix) <- NAMEAN.R <- mean(correlmatrix, na.rm =TRUE)rbar.running <- rollapply(x,WL,runningrforSTABILIZEVARIANCE,by=1,by.column=FALSE,na.pad=TRUE)rbar.running <- na.locf(rbar.running,na.rm=FALSE)rbar.running <- rev(na.locf(rev(rbar.running),na.rm=FALSE))rbar.running[samplesize==0] <- NAeffsamplesize <- samplesize/(1+(samplesize-1)*rbar.running)effsamplesize <- pmin(effsamplesize,samplesize,na.rm=TRUE) # takes care of setting the effsamplesize to 1 when sampledepth=1# and also if rbar goes negative effsamplesize gets larger than samplesize, and this brings it back down.simpleeffsamplesize <- samplesize/(1+(samplesize-1)*MEAN.R)RUNNINGadjustedchronology <- chronology*(effsamplesize*MEAN.R)^.5RUNNINGadjusted.biwgt.chronology  <- biwgt.chronology*(effsamplesize*MEAN.R)^.5SIMPLEadjustedchronology <- chronology*(simpleeffsamplesize*MEAN.R)^.5SIMPLEadjusted.biwgt.chronology <- biwgt.chronology*(simpleeffsamplesize*MEAN.R)^.5everything <- ts.union(chronology,biwgt.chronology,SIMPLEadjustedchronology,SIMPLEadjusted.biwgt.chronology,RUNNINGadjustedchronology,RUNNINGadjusted.biwgt.chronology,samplesize,effsamplesize,rbar.running)everything <- scale(everything,center=c(rep(-mean.x,6),rep(0,3)),scale=FALSE)#add back the mean to the datasetreturn(everything)}###########################################################################################function stabilizevariance2#(requires zoo)##########################################################################################stabilizevariance2 <- function(x,WL) {	runningrforSTABILIZEVARIANCE <-function (data, win.length=WL) {		cor.mat <- cor(data, use="pairwise.complete.obs")		diag(cor.mat) <- NA		obs.overlap.mat <- observationoverlap(data)		cor.mat[obs.overlap.mat < (win.length/3)] <- NA		MEAN.R <- mean(cor.mat, na.rm=TRUE)	return(MEAN.R)	}samplesize <- tssampledepth(x)chronology <- makechronology(x)biwgt.chronology <- biweightrobustmean(x)correlmatrix <- cor(x,use="pairwise.complete.obs") # could insert code for using only correls with certain observation overlapdiag(correlmatrix) <- NAMEAN.R <- mean(correlmatrix, na.rm =TRUE)rbar.running <- rapply(x,WL,runningrforSTABILIZEVARIANCE,by=1,by.column=FALSE,na.pad=TRUE)rbar.running <- na.locf(rbar.running,na.rm=FALSE)rbar.running <- rev(na.locf(rev(rbar.running),na.rm=FALSE))rbar.running[samplesize==0] <- NAeffsamplesize <- samplesize/(1+(samplesize-1)*rbar.running)effsamplesize <- pmin(effsamplesize,samplesize,na.rm=TRUE) # takes care of setting the effsamplesize to 1 when sampledepth=1# and also if rbar goes negative effsamplesize gets larger than samplesize, and this brings it back down.simpleeffsamplesize <- samplesize/(1+(samplesize-1)*MEAN.R)RUNNINGadjustedchronology <- chronology*(effsamplesize*MEAN.R)^.5RUNNINGadjusted.biwgt.chronology  <- biwgt.chronology*(effsamplesize*MEAN.R)^.5SIMPLEadjustedchronology <- chronology*(simpleeffsamplesize*MEAN.R)^.5SIMPLEadjusted.biwgt.chronology <- biwgt.chronology*(simpleeffsamplesize*MEAN.R)^.5everything <- ts.union(chronology,biwgt.chronology,SIMPLEadjustedchronology,SIMPLEadjusted.biwgt.chronology,RUNNINGadjustedchronology,RUNNINGadjusted.biwgt.chronology,samplesize,effsamplesize,rbar.running)return(everything)}###########################################################################################function stabilizevariance##########################################################################################stabilizevariance <-function(x,WL) {samplesize <- tssampledepth(x)chronology <- makechronology(x)correlmatrix <- cor(x,use="pairwise.complete.obs") # could insert code for using only correls with certain observation overlapdiag(correlmatrix) <- NAMEAN.R <- mean(correlmatrix, na.rm =TRUE)rbar.running <- rapply(x,WL,runningrforRAPPLY,by=1,by.column=FALSE,na.pad=TRUE, WINDOW=WL)effsamplesize <- samplesize/(1+(samplesize-1)*rbar.running)effsamplesize <- pmin(effsamplesize,samplesize,na.rm=TRUE) # takes care of setting the effsamplesize to 1 when sampledepth=1# and also if rbar goes negative effsamplesize gets larger than samplesize, and this brings it back down.RUNNINGadjustedchronology <- chronology*(effsamplesize*MEAN.R)^.5simpleeffsamplesize <- samplesize/(1+(samplesize-1)*MEAN.R)SIMPLEadjustedchronology <- chronology*(simpleeffsamplesize*MEAN.R)^.5everything <- ts.union(chronology,RUNNINGadjustedchronology,SIMPLEadjustedchronology,effsamplesize,samplesize,rbar.running)return(everything)}###########################################################################################function runningrforRAPPLY## efficient !##########################################################################################runningrforRAPPLY <-function (x, WL=50) {cor.mat <- cor(x, use="pairwise.complete.obs")diag(cor.mat) <- NAobs.overlap.mat <- observationoverlap(x)cor.mat[obs.overlap.mat < (WL/3)] <- NAMEAN.R <- mean(cor.mat, na.rm=TRUE)return(MEAN.R)}############################################################################################function runningcorrel##calls functions observationoverlap##correlations run from last year of chronology to first year#time series run this way also!###########################################################################################runningcorrel <- function(x,windowlength,stepsize) {Ncorrelpairs <- ceiling((tsp(x)[2]-tsp(x)[1]-windowlength)/stepsize)+1windowcorrels <- array(NA, dim=c(ncol(x),ncol(x),Ncorrelpairs))windowoverlap <- array(NA, dim=c(ncol(x),ncol(x),Ncorrelpairs))windowend <- tsp(x)[2]windowbegin <- windowend-windowlength+1i <- 1centralyears <- NArbar_simple <- NAmeansampledepth <- NA	while (windowbegin > tsp(x)[1]) {		tempwindow <- window(x,start=windowbegin,end=windowend,extend = FALSE)		windowoverlap[,,i] <- observationoverlap(tempwindow)		tempwindowcorrels  <- cor(tempwindow,use="pairwise.complete.obs")		diag(tempwindowcorrels) <- NA		tempwindowcorrels[windowoverlap[,,i]< windowlength/2] <- NA		rbar_simple[i] <- mean(tempwindowcorrels, na.rm= TRUE)		windowcorrels[,,i] <- tempwindowcorrels		#diag(windowcorrels[,,i] <- NA		centralyears[i] <- ceiling(windowbegin+windowend)/2		meansampledepth[i] <- mean(rowSums((tempwindow*0+1),na.rm=TRUE))		windowend<-windowend-stepsize		windowbegin<- windowend-windowlength+1		i<-i+1	}	return(windowcorrels, windowoverlap,centralyears,rbar_simple,meansampledepth)}############################################################################################function fastrunningcorrelforvarianceadjustment## same as slow version, but does not store all of the intermediate data matrix stuff.###calls functions observationoverlap##correlations run from first year of chronology to last year#time series run this way also!###########################################################################################fastrunningcorrelforvarianceadjustment <- function(x,windowlength,stepsize) {Ncorrelpairs <- ceiling(tsp(x)[2]-tsp(x)[1])+1tempwindowcorrels <- matrix(NA,ncol(x),ncol(x))tempwindowoverlap <- matrix(NA,ncol(x),ncol(x))windowbegin <- tsp(x)[1]-windowlength/2windowend <- windowbegin+windowlength-1i <- 1centralyears <- NArbar_simple <- NAmeansampledepth <- NA	while (windowend-windowlength/2 < tsp(x)[2]) {		tempwindow <- window(x,start=windowbegin,end=windowend,extend = FALSE)		tempwindowoverlap <- observationoverlap(tempwindow)		tempwindowcorrels  <- cor(tempwindow,use="pairwise.complete.obs")		diag(tempwindowcorrels) <- NA		tempwindowcorrels[tempwindowoverlap < windowlength/2] <- NA  # condition to get rid of correls with not enough overlap, now half window width		rbar_simple[i] <- mean(tempwindowcorrels, na.rm= TRUE)		centralyears[i] <- ceiling((windowbegin+windowend)/2)		meansampledepth[i] <- mean(rowSums((tempwindow*0+1),na.rm=TRUE))		windowbegin <- windowbegin + stepsize		windowend <- windowbegin + windowlength-1		i<-i+1	}return(centralyears,rbar_simple,meansampledepth)}############################################################################################function runningcorrelforvarianceadjustment##calls functions observationoverlap##correlations run from last year of chronology to first year#time series run this way also!###########################################################################################runningcorrelforvarianceadjustment <- function(x,windowlength,stepsize) {Ncorrelpairs <- ceiling(tsp(x)[2]-tsp(x)[1])+1windowcorrels <- array(NA, dim=c(ncol(x),ncol(x),Ncorrelpairs))windowoverlap <- array(NA, dim=c(ncol(x),ncol(x),Ncorrelpairs))windowbegin <- tsp(x)[1]-windowlength/2windowend <- windowbegin+windowlength-1i <- 1centralyears <- NArbar_simple <- NAmeansampledepth <- NA	while (windowend-windowlength/2 < tsp(x)[2]) {		tempwindow <- window(x,start=windowbegin,end=windowend,extend = FALSE)		windowoverlap[,,i] <- observationoverlap(tempwindow)		tempwindowcorrels  <- cor(tempwindow,use="pairwise.complete.obs")		diag(tempwindowcorrels) <- NA		tempwindowcorrels[windowoverlap[,,i]< windowlength/2] <- NA  # condition to get rid of correls with not enough overlap		rbar_simple[i] <- mean(tempwindowcorrels, na.rm= TRUE)		windowcorrels[,,i] <- tempwindowcorrels		#diag(windowcorrels[,,i] <- NA		centralyears[i] <- ceiling(windowbegin+windowend)/2		meansampledepth[i] <- mean(rowSums((tempwindow*0+1),na.rm=TRUE))		windowbegin <- windowbegin + stepsize		windowend <- windowbegin + windowlength-1		i<-i+1	}	return(windowcorrels, windowoverlap,centralyears,rbar_simple,meansampledepth)}###########################################################################################function EPS##########################################################################################EPS <- function(N=samplesize,r=rbar) {(N*r)/(1+N*r-r)}###########################################################################################function SSS  ***untested****##########################################################################################SSS <- function (n=maxsampledepth,N=sampledepthsubset,r=rbar) {(n*(1+(N-1)*r))/(N*(1+(n-1)*r))}###########################################################################################function observationoverlap## efficient !##########################################################################################observationoverlap <- function(x) {	presencematrix <- 0*(as.matrix(x))+1	presencematrix[is.na(presencematrix)] <- 0	overlapmatrix <- t(presencematrix)%*%presencematrix	overlapmatrix}###########################################################################################function observationoverlap## damn slow !############################################################################################observationoverlap <- function(x) {##size <- ncol(x)#overlapmatrix <- matrix(NA,size,size)#presencematrix <- 0*(as.data.frame(x))+1##	for (i in 1:size) {#		for (j in 1:size)#			overlapmatrix[i,j]<-sum(presencematrix[,i]*presencematrix[,j],na.rm=TRUE)#	}#overlapmatrix#}############################################################################################function biweightrobustmean##if using on a data.frame call function as: biweightrobustmean(as.matrix(x))###########################################################################################biweightrobustmean <- function(x) {	tempmean <- rep(NA,nrow(x))	sampledepth<-rowSums(0*x+1, na.rm=TRUE)	rawmedian<- apply(x,1,median,na.rm=TRUE)	x_norm <- (x-rawmedian)/apply(x,1,mad,na.rm=TRUE) # here using median absolute deviation and initial estimate with median instead, maybe more robust	for (i in 1:3) {		x_norm[abs(x_norm) > 6.08] <- 6.08      # special consideration for any extreme value over 6.08 stdev away to get weight of 6.08 stdev which is zero		xweights <- ((1-(x_norm/6.08)^2)^2)					for (j in 1:nrow(x)) {			tempmean[j] <- weighted.mean(x[j,],xweights[j,], na.rm=TRUE)			}				x_norm <- (x-tempmean)/apply(x,1,mad, na.rm=TRUE) # here using median absolute deviation	}	tempmean[which(sampledepth==1)]<-rawmedian[which(sampledepth==1)]	ifelse(is.ts(x)==TRUE, robustmean <- ts(tempmean, start = start(x)[1]), robustmean <- tempmean)robustmean}###########################################################################################function normalizewindow##########################################################################################normalizewindow <- function(x, start, end) {scale(x,center=colMeans(window(x,start,end)),scale=apply(window(x,start,end),2,sd))}###########################################################################################function makechronology##########################################################################################makechronology <- function(x) {ts(rowMeans(x,na.rm=TRUE),start=start(x)[1])}###########################################################################################function importtreeringdata##########################################################################################importtreeringdata <- function(x) {		tempimport <- scan(file=x,sep = "\t", skip=1, na.strings="-9.9900")	tempimportnames <- scan(file=x,sep = "\t", skip=0, na.strings="-9.9900",nlines=1, what="character")		treeringdata <- matrix(tempimport,ncol=length(tempimportnames), byrow=TRUE)	colnames(treeringdata) <- tempimportnames	tstreeringdata <- ts(treeringdata[,-1], start=treeringdata[1,1])	tstreeringdata}###########################################################################################function tssampledepth##########################################################################################tssampledepth <- function(x) {		tssampledepth <- rowSums(0*x+1, na.rm=TRUE)	if (is.ts(x)) { tssampledepth <- ts(tssampledepth, start=start(x), frequency=frequency(x))}		return(tssampledepth)}###########################################################################################function addsampledepth##########################################################################################addsampledepth <-function(x) {		par(new=TRUE)		if(is.ts(x) && !is.mts(x)) {sampledepth <- x }	if(is.mts(x)) { sampledepth <- tssampledepth(x) }	ts.plot(sampledepth, gpars=list(axes=FALSE, ann=FALSE, col="black", lty=1))	axis(4)	mtext(" Sample Size", side = 4, adj=0, line=-1.2)}###randomstuff##runningcorrel(xtscut,50,1)->f#cbind(f$centralyears,f$rbar_simple,f$meansampledepth)->fts###